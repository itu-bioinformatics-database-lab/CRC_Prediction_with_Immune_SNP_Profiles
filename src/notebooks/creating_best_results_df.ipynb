{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69591710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import yaml\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import sys\n",
    "main_path = \"../..\"\n",
    "sys.path.append(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cefaf036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_path = f\"{main_path}/data/processed/model_scores\"\n",
    "os.makedirs(model_scores_path, exist_ok=True)\n",
    "\n",
    "with open(f\"{model_scores_path}/best_scores.pkl\", 'rb') as f:\n",
    "    metric_results = pickle.load(f)\n",
    "    \n",
    "with open(f\"{model_scores_path}/colonflag_scores.pkl\", 'rb') as f:\n",
    "    cf_metric_results = pickle.load(f)\n",
    "\n",
    "with open(f\"{main_path}/config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "models = ['LR', 'RF', 'SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186cd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "def find_mean_auc(result):\n",
    "    tprs = [i['tpr'] for i in result]\n",
    "    mean_tpr = np.array(tprs).mean(axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    auc_mean = round(auc(mean_fpr, mean_tpr), 2)\n",
    "    \n",
    "    return auc_mean, mean_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65d3aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach, model, f1, auc\n",
    "best_metric_results = []\n",
    "mean_tprs = {}\n",
    "\n",
    "for appr in config['approach_names']:\n",
    "    best_model = [0, None]\n",
    "    for m in models:\n",
    "        score = np.mean(metric_results[appr.lower()][m.lower()]['f1_scores'])\n",
    "        if score > best_model[0]:\n",
    "            best_model[0] = score\n",
    "            best_model[1] = m\n",
    "    \n",
    "    result = metric_results[appr.lower()][best_model[1].lower()]['rocauc']\n",
    "    auc_mean, mean_tpr = find_mean_auc(result)\n",
    "    \n",
    "    best_metric_results.append([appr, best_model[1], best_model[0], auc_mean])\n",
    "    mean_tprs[appr] = {}\n",
    "    mean_tprs[appr][best_model[1]] = mean_tpr\n",
    "    \n",
    "    # colonflag\n",
    "    f1_mean = np.mean(cf_metric_results[appr.lower()]['f1_scores'])\n",
    "    \n",
    "    result = cf_metric_results[appr.lower()]['rocauc']\n",
    "    auc_mean, mean_tpr = find_mean_auc(result)\n",
    "    \n",
    "    best_metric_results.append([appr, 'CF', f1_mean, auc_mean])\n",
    "    mean_tprs[appr]['CF'] = mean_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa49907",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(best_metric_results, columns=['approach', 'model', 'f1_score', 'auc_score'])\n",
    "results_df.to_csv(f'{model_scores_path}/approach_model_f1_auc.csv', index=False)\n",
    "\n",
    "with open(f'{model_scores_path}/mean_tprs_comparison.pkl', 'wb') as f:\n",
    "    pickle.dump(mean_tprs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('coloncancer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17d88d7bb09b9e5610217549f5ba06556c09b361bd5cd1a1140320cbcbaad1ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
